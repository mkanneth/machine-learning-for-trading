{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import random, uniform, dirichlet, choice\n",
    "from numpy.linalg import inv\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "import pandas_datareader.data as web\n",
    "from statsmodels.regression.rolling import RollingOLS\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "from scipy.optimize import minimize\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "from talib import RSI, BBANDS, MACD\n",
    "\n",
    "from pykalman import KalmanFilter\n",
    "import pywt\n",
    "\n",
    "from alphalens.utils import get_clean_factor_and_forward_returns\n",
    "from alphalens.performance import *\n",
    "from alphalens.plotting import *\n",
    "from alphalens.tears import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_STORE = '../data/assets.h5'\n",
    "START = 2000\n",
    "END = 2018\n",
    "idx = pd.IndexSlice\n",
    "with pd.HDFStore(DATA_STORE) as store:\n",
    "    prices = (store['quandl/wiki/prices']\n",
    "              .loc[idx[str(START):str(END), :], 'adj_close']\n",
    "              .unstack('ticker'))\n",
    "    stocks = store['us_equities/stocks'].loc[:, ['marketcap', 'ipoyear', 'sector']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_historical_returns(input_df,data_frequency=\"D\",return_intervals=[1,4]):\n",
    "    #input_df: Dataframe with daily/weekly/monthly stock price data\n",
    "    #return_invervals should be in specified in the increment the input_df is\n",
    "    #returns are normalized to the data_frequence (i.e. daily data is normalized to daily returns, monthly to per month, etc)\n",
    "\n",
    "    df = input_df.copy()\n",
    "    columns = df.columns\n",
    "    returns = pd.DataFrame()\n",
    "\n",
    "    for i in return_intervals:\n",
    "        for col in columns:\n",
    "            returns[f'{col}_{i}{data_frequency}_return'] = df[col].pct_change(i).add(1).pow(1/i).sub(1)\n",
    "\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For this function to work, x_df and y_df need to have datetime indexes with the same dates to join on\n",
    "def rolling_OLS(x_df,y_df,rolling_window):\n",
    "\n",
    "    if isinstance(x_df.index,pd.DatetimeIndex):\n",
    "        pass\n",
    "    else:\n",
    "        x_df.index = x_df.index.to_timestamp()\n",
    "\n",
    "    if isinstance(y_df.index,pd.DatetimeIndex):\n",
    "        pass\n",
    "    else:\n",
    "        y_df.index = y_df.index.to_timestamp()\n",
    "   \n",
    "    \n",
    "    y_df.index.name = 'date'\n",
    "    x_df.index.name = 'date'\n",
    "\n",
    "    y_name = y_df.columns[0]\n",
    "    regression_df = x_df.join(y_df).sort_index()\n",
    "\n",
    "\n",
    "    regression = RollingOLS(endog=regression_df[y_name],\n",
    "                            exog=sm.add_constant(regression_df.drop(y_name,axis=1)),\n",
    "                            window=min(rolling_window,len(regression_df)-1)\n",
    "                            ).fit(params_only=True)\n",
    "    \n",
    "    params = regression.params.drop('const',axis=1)\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How to pull Fama French Data\n",
    "factor_data = web.DataReader('F-F_Research_Data_5_Factors_2x3', 'famafrench', start='2000')[0].drop('RF', axis=1)\n",
    "factor_data.index = factor_data.index.to_timestamp()\n",
    "factor_data = factor_data.div(100).resample('M').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BBands doesn't handle N/A well, need to drop them or fill in na witha average of edges\n",
    "up, mid, low = BBANDS(prices['AAPL'].dropna(), timeperiod=10, nbdevup=2, nbdevdn=2, matype=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relative strength index, >=70 means overbought, <=30 means underbought\n",
    "rsi = RSI(prices['AAPL'].dropna(),timeperiod=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MACD is simply the difference between the fast EMA and slow EMA\n",
    "#macdsignal is just the moving average of the macd, the time periods is based on the \"signalperiod\" variable\n",
    "#macdhist is just the difference between the macd and the macdsignla\n",
    "\n",
    "macd, macdsignal, macdhist = MACD(prices['AAPL'].dropna(), fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "sns.lineplot(macdsignal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rolling Pandas Functions\n",
    "\n",
    "#Below is an example of what it looks like, rolling provides a window of values to use in another function (mean in this case)\n",
    "#df.rolling(window=...).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kalman Filters\n",
    "\n",
    "\n",
    "kf = KalmanFilter(transition_matrices = [1],\n",
    "                  observation_matrices = [1],\n",
    "                  initial_state_mean = 0,   #suggested in book\n",
    "                  initial_state_covariance = 1, #suggested in book\n",
    "                  observation_covariance=1,\n",
    "                  transition_covariance=.01)\n",
    "\n",
    "#prices_df would be daily prices of a stock\n",
    "state_means, _ = kf.filter(prices_kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOLDING_PERIODS = (5, 10, 21, 42)\n",
    "QUANTILES = 5\n",
    "\n",
    "#Factor_df can be a df, but needs to have a single factor. Index needs to be multiindex, first \"date\" and then \"asset\". \"date\" need to have a timezone to work\n",
    "#so it will be three \"columns\", total, two of them are the indexes and one is the factor that you are analyzing\n",
    "\n",
    "#prices df needs to be a \"wide\" df, with each asset's prices being a new column. The index needs to be a singel datetime index WITH timezone. \n",
    "factor_df = pd.read_csv('sample_factor_data.csv')\n",
    "\n",
    "#reformatting the columns to be indexes\n",
    "factor_df = factor_df.set_index([pd.to_datetime(factor_df['date']),'asset'])\n",
    "factor_df.drop('date',axis=1,inplace=True)\n",
    "\n",
    "prices = pd.read_csv('stock_prices_daily.csv')\n",
    "prices['Unnamed: 0'] = pd.to_datetime(prices['Unnamed: 0'])\n",
    "prices = prices.set_index(['Unnamed: 0'])\n",
    "\n",
    "alphalens_data = get_clean_factor_and_forward_returns(factor=factor_df,\n",
    "                                                      prices=prices,\n",
    "                                                      periods=HOLDING_PERIODS,\n",
    "                                                      quantiles=QUANTILES)\n",
    "\n",
    "\n",
    "#The below simply plots a potential cumulative returns graphs based on quantiles, you need to specify a single period from your above quantile options twice below\n",
    "mean_return_by_q_daily, std_err = mean_return_by_quantile(alphalens_data, by_date=True)\n",
    "plot_cumulative_returns_by_quantile(mean_return_by_q_daily['5D'], period='5D', freq=None)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_return_by_q_daily, std_err = mean_return_by_quantile(alphalens_data, by_date=True)\n",
    "plot_cumulative_returns_by_quantile(mean_return_by_q_daily['5D'], period='5D', freq=None)\n",
    "plt.tight_layout()\n",
    "\n",
    "#The resulting plot shows what an ideal factor should have, i.e. quantile 5 does the best and 1 does the worst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic = factor_information_coefficient(alphalens_data)\n",
    "\n",
    "plot_ic_ts(ic[['5D']])\n",
    "plt.tight_layout()\n",
    "sns.despine();\n",
    "\n",
    "#Below is alternative plot, comment out above to see this one \n",
    "# ic_by_year = ic.resample('A').mean()\n",
    "# ic_by_year.index = ic_by_year.index.year\n",
    "# ic_by_year.plot.bar(figsize=(14, 6))\n",
    "# plt.tight_layout();\n",
    "\n",
    "#The above plots the information cooeficient but takes the inital alphalens data as the input so can only be for one factor at a time\n",
    "#NOTE: This is NOT the IR (Information Ratio), just a proxy metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You need to run the top cell that generates the prices df first\n",
    "\n",
    "weekly_returns = prices[['AAPL','ZBRA','GOOG','NDAQ','GILD','DOW']].resample('W').last().pct_change().dropna(how='all')\n",
    "weekly_returns = weekly_returns.dropna(axis=1)\n",
    "\n",
    "n_obs, n_assets = weekly_returns.shape\n",
    "n_assets, n_obs\n",
    "\n",
    "#Calculate annulaization factor\n",
    "periods_per_year = round(weekly_returns.resample('YE').size().mean())\n",
    "\n",
    "NUM_PF = 100000 # no of portfolios to simulate\n",
    "\n",
    "#Create initial weights and normalize to sum to 1\n",
    "x0 = uniform(0, 1, n_assets)\n",
    "x0 /= np.sum(np.abs(x0))\n",
    "\n",
    "#Calculate mean and covariance matrices\n",
    "mean_returns = weekly_returns.mean()\n",
    "cov_matrix = weekly_returns.cov()\n",
    "\n",
    "#The precision matrix is the inverse of the covariance matrix:\n",
    "precision_matrix = pd.DataFrame(inv(cov_matrix), index=weekly_returns.columns, columns=weekly_returns.columns)\n",
    "\n",
    "#Obtain risk free rate\n",
    "treasury_10yr_monthly = (web.DataReader('DGS10', 'fred', weekly_returns.index.min().year, weekly_returns.index.max().year)\n",
    "                         .resample('ME')\n",
    "                         .last()\n",
    "                         .div(periods_per_year)\n",
    "                         .div(100)\n",
    "                         .squeeze())\n",
    "\n",
    "rf_rate = treasury_10yr_monthly.mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def simulate_portfolios(mean_ret, rf_rate=rf_rate, short=True):\n",
    "\n",
    "    #The initial alpha array tells the weights how to concentrate samples at random for each array it generates\n",
    "    alpha = np.full(shape=n_assets, fill_value=.05)\n",
    "    weights = dirichlet(alpha=alpha, size=NUM_PF)\n",
    "    \n",
    "    if short:\n",
    "        weights *= choice([-1, 1], size=weights.shape)\n",
    "\n",
    "    #Dot product of weights and returns, so just weighted average\n",
    "    returns = weights @ mean_ret.values + 1\n",
    "\n",
    "    #nth root to normalize returns\n",
    "    returns = returns ** periods_per_year - 1\n",
    "\n",
    "\n",
    "    std = (weights @ weekly_returns.T).std(1)\n",
    "    std *= np.sqrt(periods_per_year)\n",
    "\n",
    "\n",
    "    sharpe = (returns - rf_rate) / std\n",
    "    return pd.DataFrame({'Annualized Standard Deviation': std,\n",
    "                         'Annualized Returns': returns,\n",
    "                         'Sharpe Ratio': sharpe}), weights\n",
    "\n",
    "\n",
    "#Additional optional functions to run\n",
    "def portfolio_std(wt, rt=None, cov=None):\n",
    "    \"\"\"Annualized PF standard deviation\"\"\"\n",
    "    return np.sqrt(wt @ cov @ wt * periods_per_year)\n",
    "\n",
    "def portfolio_returns(wt, rt=None, cov=None):\n",
    "    \"\"\"Annualized PF returns\"\"\"\n",
    "    return (wt @ rt + 1) ** periods_per_year - 1\n",
    "\n",
    "\n",
    "def portfolio_performance(wt, rt, cov):\n",
    "    \"\"\"Annualized PF returns & standard deviation\"\"\"\n",
    "    r = portfolio_returns(wt, rt=rt)\n",
    "    sd = portfolio_std(wt, cov=cov)\n",
    "    return r, sd\n",
    "\n",
    "#Negative so we can optimize by finding smallest (which is best actual)\n",
    "def neg_sharpe_ratio(weights, mean_ret, cov):\n",
    "    r, sd = portfolio_performance(weights, mean_ret, cov)\n",
    "    return -(r - rf_rate) / sd\n",
    "\n",
    "weight_constraint = {'type': 'eq', \n",
    "                     'fun': lambda x: np.sum(np.abs(x))-1}\n",
    "\n",
    "def max_sharpe_ratio(mean_ret, cov, short=False):\n",
    "    return minimize(fun=neg_sharpe_ratio,\n",
    "                    x0=x0,\n",
    "                    args=(mean_ret, cov),\n",
    "                    method='SLSQP',\n",
    "                    bounds=((-1 if short else 0, 1),) * n_assets,\n",
    "                    constraints=weight_constraint,\n",
    "                    options={'tol':1e-10, 'maxiter':1e4})\n",
    "\n",
    "def min_vol(mean_ret, cov, short=False):\n",
    "    bounds = ((-1 if short else 0, 1),) * n_assets\n",
    "\n",
    "    return minimize(fun=portfolio_std,\n",
    "                    x0=x0,\n",
    "                    args=(mean_ret, cov),\n",
    "                    method='SLSQP',\n",
    "                    bounds=bounds,\n",
    "                    constraints=weight_constraint,\n",
    "                    options={'tol': 1e-10, 'maxiter': 1e4})\n",
    "\n",
    "def min_vol_target(mean_ret, cov, target, short=False):\n",
    "\n",
    "    def ret_(wt):\n",
    "        return portfolio_returns(wt, mean_ret)\n",
    "\n",
    "    constraints = [{'type': 'eq',\n",
    "                    'fun': lambda x: ret_(x) - target},\n",
    "                   weight_constraint]\n",
    "\n",
    "    bounds = ((-1 if short else 0, 1),) * n_assets\n",
    "    return minimize(portfolio_std,\n",
    "                    x0=x0,\n",
    "                    args=(mean_ret, cov),\n",
    "                    method='SLSQP',\n",
    "                    bounds=bounds,\n",
    "                    constraints=constraints,\n",
    "                    options={'tol': 1e-10, 'maxiter': 1e4})\n",
    "\n",
    "def efficient_frontier(mean_ret, cov, ret_range, short=False):\n",
    "    return [min_vol_target(mean_ret, cov, ret) for ret in ret_range]\n",
    "\n",
    "\n",
    "simul_perf, simul_wt = simulate_portfolios(mean_returns, short=False)\n",
    "simul_max_sharpe = simul_perf.iloc[:, 2].idxmax()\n",
    "max_sharpe_pf = max_sharpe_ratio(mean_returns, cov_matrix, short=False)\n",
    "max_sharpe_perf = portfolio_performance(max_sharpe_pf.x, mean_returns, cov_matrix)\n",
    "r, sd = max_sharpe_perf\n",
    "\n",
    "min_vol_pf = min_vol(mean_returns, cov_matrix, short=False)\n",
    "min_vol_perf = portfolio_performance(min_vol_pf.x, mean_returns, cov_matrix)\n",
    "\n",
    "\n",
    "ret_range = np.linspace(simul_perf.iloc[:, 1].min(), simul_perf.iloc[:, 1].max(), 50)\n",
    "eff_pf = efficient_frontier(mean_returns, cov_matrix, ret_range, short=True)\n",
    "eff_pf = pd.Series(dict(zip([p['fun'] for p in eff_pf], ret_range)))\n",
    "\n",
    "\n",
    "#Below is all code for plot\n",
    "fig, ax = plt.subplots()\n",
    "simul_perf.plot.scatter(x=0, y=1, c=2, ax=ax, cmap='Blues',alpha=0.25, \n",
    "                        figsize=(14, 9), colorbar=True)\n",
    "\n",
    "eff_pf[eff_pf.index.min():].plot(linestyle='--', lw=2, ax=ax, c='k',\n",
    "                                 label='Efficient Frontier')\n",
    "\n",
    "r, sd = max_sharpe_perf\n",
    "ax.scatter(sd, r, marker='*', color='k', s=500, label='Max Sharpe Ratio PF')\n",
    "\n",
    "r, sd = min_vol_perf\n",
    "ax.scatter(sd, r, marker='v', color='k', s=200, label='Min Volatility PF')\n",
    "\n",
    "kelly_wt = precision_matrix.dot(mean_returns).clip(lower=0).values\n",
    "kelly_wt /= np.sum(np.abs(kelly_wt))\n",
    "r, sd = portfolio_performance(kelly_wt, mean_returns, cov_matrix)\n",
    "ax.scatter(sd, r, marker='D', color='k', s=150, label='Kelly PF')\n",
    "\n",
    "std = weekly_returns.std()\n",
    "std /= std.sum()\n",
    "r, sd = portfolio_performance(std, mean_returns, cov_matrix)\n",
    "ax.scatter(sd, r, marker='X', color='k', s=250, label='Risk Parity PF')\n",
    "\n",
    "r, sd = portfolio_performance(np.full(n_assets, 1/n_assets), mean_returns, cov_matrix)\n",
    "ax.scatter(sd, r, marker='o', color='k', s=200, label='1/n PF')\n",
    "\n",
    "\n",
    "ax.legend(labelspacing=0.8)\n",
    "ax.set_xlim(0, eff_pf.max()+.4)\n",
    "ax.set_title('Mean-Variance Efficient Frontier', fontsize=16)\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\n",
    "ax.xaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\n",
    "sns.despine()\n",
    "fig.tight_layout();\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
